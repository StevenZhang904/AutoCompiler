from langchain_openai import ChatOpenAI
from langchain.agents import Tool, AgentType, AgentExecutor, create_react_agent, initialize_agent
from langchain_core.agents import AgentAction, AgentFinish
from langchain.prompts import PromptTemplate
from langchain.schema import StrOutputParser
from google_search import search_agent
from langchain_core.tools.render import render_text_description
from langchain_core.utils.function_calling import convert_to_openai_tool
from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser
from typing import List, Dict, Any, Tuple
from config import *
import logging
import time 
import json
import re
import ast
import os

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

file_handler = logging.FileHandler('Reconcile.log',mode='a')
file_handler.setLevel(logging.INFO)
formatter = logging.Formatter("%(asctime)s %(name)s:%(levelname)s:%(message)s")
file_handler.setFormatter(formatter)

logger.addHandler(file_handler)

llm1 = OPENAI_MODEL
llm2 = ANTHROPIC_MODEL
llm3 = DEEPSEEK_MODEL

os.environ["LANGCHAIN_TRACING_V2"] = LANGCHAIN_TRACING_V2
os.environ["LANGCHAIN_API_KEY"] = LANGCHAIN_API_KEY
os.environ["LANGCHAIN_PROJECT"] = LANGCHAIN_PROJECT_RECONCILE

def trans_confidence(x):
    x = float(x)
    if x <= 0.6: return 0.1
    if 0.8 > x > 0.6: return 0.3
    if 0.9 > x >= 0.8: return 0.5
    if 1 > x >= 0.9: return 0.8
    if x == 1: return 1

def parse_json(model_output):
    if type(model_output) is dict:
        return model_output
    elif type(model_output) is not str:
        model_output = str(model_output)
    try:
        model_output = model_output.replace("\n", " ")
        model_output = re.search('({.+})', model_output).group(0)
        model_output = re.sub(r"(\w)'(\w|\s)", r"\1\\'\2", model_output)
        result = ast.literal_eval(model_output)
    except (SyntaxError, NameError, AttributeError):
        return "ERR_SYNTAX"
    return result

def count_common_words(str1, str2):
    set1 = set(str1.split())
    set2 = set(str2.split())
    return len(set1 & set2)

def clean_output(all_results, rounds):
    llm1_output, llm2_output, llm3_output = f"{llm1}_output_{rounds}", f'{llm2}_output_{rounds}', f'{llm3}_output_{rounds}'
    for result in all_results:
        for output in [llm1_output, llm2_output, llm3_output]:
            if output in result:
                try:
                    if result[output]['output'].startswith('```json'):
                        result[output]['output'] = result[output]['output'][7:-3]
                    str2dic = json.loads(result[output]['output'])
                    for key in str2dic.keys():
                        result[output][key] = str2dic[key]
                except:
                    pass

                if 'reasoning' not in result[output]:
                    result[output]['reasoning'] = ""
                elif type(result[output]['reasoning']) is list:
                    result[output]['reasoning'] = " ".join(result[output]['reasoning'])

                if 'solution' not in result[output]:
                    result[output]['solution'] = "sorry, i don't know."
                elif type(result[output]['solution']) is list:
                    result[output]['solution'] = " ".join(result[output]['solution'])

                if 'confidence_level' not in result[output] or not result[output]['confidence_level']:
                    result[output]['confidence_level'] = 0.0
                else:
                    if type(result[output]['confidence_level']) is str and "%" in result[output]['confidence_level']:
                            result[output]['confidence_level'] = float(result[output]['confidence_level'].replace("%","")) / 100
                    else:
                        try:
                            result[output]['confidence_level'] = float(result[output]['confidence_level'])
                        except:
                            print(result[output]['confidence_level'])
                            result[output]['confidence_level'] = 0.0

    return all_results


def parse_output(all_results, rounds, threshold=5):
    round = f'_output_{rounds}'

    for result in all_results:
        certainty_vote = {}
        for output in [llm1, llm2, llm3]:
            if output+round in result:
                result[f'{output}_pred_{rounds}'] = result[output+round]['solution']
                result[f'{output}_exp_{rounds}'] = f"I think the answer is {result[output+round]['solution']} because {result[output+round]['reasoning']} My confidence level is {result[output+round]['confidence_level']}."

                if result[output+round]['solution'] not in certainty_vote:
                    if certainty_vote == {}:
                        certainty_vote[result[output+round]['solution']] = trans_confidence(result[output+round]['confidence_level'])
                    else:
                        keys = list(certainty_vote.keys())
                        for key in keys:
                            if count_common_words(key, result[output+round]['solution']) >= threshold:
                                certainty_vote[key] += trans_confidence(result[output+round]['confidence_level'])
                            else:
                                certainty_vote[result[output+round]['solution']] = trans_confidence(result[output+round]['confidence_level'])
                            break
                else:
                    certainty_vote[result[output+round]['solution']] += trans_confidence(result[output+round]['confidence_level'])
        
        if "sorry, i don't know." in certainty_vote:
            del certainty_vote["sorry, i don't know."]

        if all(item+round in result for item in [llm1, llm2, llm3]):
            result[f'vote_{rounds}'] = [result[f'{llm1}_pred_{rounds}'], result[f'{llm2}_pred_{rounds}'], result[f'{llm3}_pred_{rounds}']]
            result[f'exps_{rounds}'] = [result[f'{llm1}_exp_{rounds}'], result[f'{llm2}_exp_{rounds}'], result[f'{llm3}_exp_{rounds}']]
            result[f'weighted_vote_{rounds}'] = certainty_vote
            result[f'weighted_max_{rounds}'] = max(certainty_vote, key=certainty_vote.get)
            result[f'debate_prompt_{rounds}'] = ''
            count = len(certainty_vote.keys())
            if count == 3:
                for key in certainty_vote.keys():
                    result[f'debate_prompt_{rounds}'] += f"One agent's solution: {key}.\n"
            elif count == 2:
                result[f'debate_prompt_{rounds}'] += f"Majority agents' solution: {result[f'weighted_max_{rounds}']}.\nOne agent's solution: {min(certainty_vote, key=certainty_vote.get)}."
            else:
                result[f'debate_prompt_{rounds}'] += f"All agents' solution: {result[f'weighted_max_{rounds}']}"

    return all_results



class SingleAgent:
    def __init__(self, base_url, model_name, api_key):
        self.base_url = base_url
        self.model_name = model_name
        self.api_key = api_key

    def create_agent(self, template):
        prompt = PromptTemplate.from_template(template=template)
        tools = self.create_tools()
        llm = ChatOpenAI(
            base_url=self.base_url,
            model=self.model_name,
            api_key=self.api_key,
            temperature=1
        )
        prompt = prompt.partial(
            tools=render_text_description(tools)
        )
        llm_with_tools = llm.bind(tools=[convert_to_openai_tool(tool) for tool in tools])
        agent = prompt | llm_with_tools | OpenAIToolsAgentOutputParser()
        return AgentExecutor(agent=agent, tools=tools, verbose=False, handle_parsing_errors=True,max_iterations=10,max_execution_time=30)


    def create_tools(self):
        return [
            Tool(
                name='Search_Agent',
                func=search_agent,
                description=search_agent.__doc__
            )
        ]
    
    def generate_answer(self, template, question, chat_history=None):
        agent = self.create_agent(template=template)
        if chat_history is None:
            output = agent.invoke({'input':question})
        else:
            output = agent.invoke({'input':question,'chat_history':chat_history})

        if output:
            if ("{" not in output or "}" not in output) and type(output) != dict:
                print(type(output))
                raise ValueError("cannot find { or } in the model output.")
            result = parse_json(output)
            if result == "ERR_SYNTAX":
                raise ValueError("incomplete JSON format.")
        return result
    
    def debate(self, template, question, results, rounds):
        for result in results:
            if (f'{self.model_name}_output_{rounds}' not in result) and (f'debate_prompt_{rounds-1}' in result) and len(result[f'debate_prompt_{rounds-1}']):
                chat_history = result[f'debate_prompt_{rounds-1}']
                res = self.generate_answer(template=template, question=question, chat_history=chat_history)
                result[f'{self.model_name}_output_{rounds}'] = res
        return results

class Reconcile:
    def __init__(self):
        self.logger = []

    def reconcile(self, mistake):
        """
        Reconcile with multiple agents to solve errors encountered during compilation.
        @param mistake: the mistake in the compilation process. 
        """
        rounds = 2
        # Pharse1 : Initial Response Generation
        time.sleep(1)
        gpt = SingleAgent(
            base_url=OPENAI_BASE_URL,
            model_name=OPENAI_MODEL,
            api_key=OPENAI_API_KEY
        )
        gpt_result = []
        tmp = {}
        tmp['prediction'] = gpt.generate_answer(template=template1,question=mistake)
        gpt_result.append(tmp)

        claude = SingleAgent(
            base_url=ANTHROPIC_BASE_URL,
            model_name=ANTHROPIC_MODEL,
            api_key=ANTHROPIC_API_KEY
        )
        claude_result = []
        tmp = {}
        tmp['prediction'] = claude.generate_answer(template=template1,question=mistake)
        claude_result.append(tmp)

        deepseek = SingleAgent(
            base_url=DEEPSEEK_BASE_URL,
            model_name=DEEPSEEK_MODEL,
            api_key=DEEPSEEK_API_KEY
        )
        deepseek_result = []
        tmp = {}
        tmp['prediction'] = deepseek.generate_answer(template=template1,question=mistake)
        deepseek_result.append(tmp)

        all_results = []
        for c, g, d in zip(gpt_result, claude_result, deepseek_result):
            all_results.append({
                f'{gpt.model_name}_output_0':g['prediction'],
                f'{claude.model_name}_output_0':c['prediction'],
                f'{deepseek.model_name}_output_0':d['prediction']
            })
        
        all_results = clean_output(all_results, rounds=0)
        all_results = parse_output(all_results, rounds=0, threshold=30)
        logging.info("[+] ------ Initial Round Discussion ------")

        # Pharse2: Multi-Round Discussion
        for round in range(1, rounds+1):
            logging.info(f"[-] ------ Round {round} Discussion ------")
            for agent in [claude, gpt, deepseek]:
                all_results = agent.debate(template=template2, question=mistake, results=all_results, rounds=round)
                time.sleep(1)
            all_results = clean_output(all_results, round)
            all_results = parse_output(all_results, round, threshold=30)
            logging.info(f"[+] ------- Round {round} Discussion Done ------")
        
        self.logger.append([
            mistake,
            all_results
        ])
        return all_results[0][f'weighted_max_{rounds}']
